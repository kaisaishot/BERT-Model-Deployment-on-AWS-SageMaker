{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T17:27:51.657810Z",
     "iopub.status.busy": "2026-01-07T17:27:51.657539Z",
     "iopub.status.idle": "2026-01-07T17:35:51.269183Z",
     "shell.execute_reply": "2026-01-07T17:35:51.268419Z",
     "shell.execute_reply.started": "2026-01-07T17:27:51.657777Z"
    },
    "id": "m2SYR8dfYNHu",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492b3a03c39d4877b022c6bd8753a68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a58d32749c4b448bea706f7f7d2777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aa_dataset-tickets-multi-lang-5-2-50-ver(…):   0%|          | 0.00/26.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863c40cc15a84e6ea3185e7f2a1754e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)set-tickets-german_normalized_50_5_2.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e2b2d859cf44d6b15a86786a9fd009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset-tickets-multi-lang-4-20k.csv:   0%|          | 0.00/18.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4a804e9e0241d99881774e3fb3a080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/61765 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e426ab42fccf460c9c8cf7d749e37e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/61765 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6491fde349f44e10bf6cc91acf885460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/28261 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution (train):\n",
      "Counter({'Technical Support': 6476, 'Customer Service': 3471, 'Billing and Payments': 2307, 'Sales and Pre-Sales': 655, 'General Inquiry': 340})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 17:28:15.737798: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767806895.932596      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767806895.986891      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767806896.461053      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767806896.461100      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767806896.461103      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767806896.461106      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3ca2c6fca84f2d8d687ba0a8a80b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b0759982b74b58b5c0a717b4fec3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587ee80d64d94651941b58c2046c452a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d208b54269624199a97b65a6c1378afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5c41d2799842caa2a10385ae11620e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.9229\n",
      "Epoch 2 | Loss: 0.7529\n",
      "Epoch 3 | Loss: 0.5283\n",
      "Epoch 4 | Loss: 0.3476\n",
      "Peak GPU Memory during training: 2944.83 MB\n",
      "memory used: 1054.59 MB\n",
      "Inference Time: 9.88s\n",
      "Discriminative BERT classifier accuracy: 0.7960\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    dataset = load_dataset(\"Tobi-Bueck/customer-support-tickets\")\n",
    "    ds = dataset[\"train\"]\n",
    "\n",
    "    # Filter only English tickets\n",
    "    ds = ds.filter(lambda ex: ex[\"language\"] == \"en\")\n",
    "\n",
    "    # Select only 5 departments \"Technical Support\", \"Customer Service\", \"Billing and Payments\",\n",
    "    #\"Sales and Pre-Sales\", \"General Inquiry\",\n",
    "\n",
    "    target_queues = [\n",
    "        \"Technical Support\",\n",
    "        \"Customer Service\",\n",
    "        \"Billing and Payments\",\n",
    "        \"Sales and Pre-Sales\",\n",
    "        \"General Inquiry\",\n",
    "    ]\n",
    "\n",
    "    ds = ds.filter(lambda ex: ex[\"queue\"] in target_queues)\n",
    "\n",
    "    # Shuffle and split train/val/test\n",
    "    ds = ds.shuffle(seed=RANDOM_SEED)\n",
    "    train_test = ds.train_test_split(test_size=0.2, seed=RANDOM_SEED)\n",
    "    test_valid = train_test[\"test\"].train_test_split(test_size=0.5, seed=RANDOM_SEED)\n",
    "\n",
    "    train_ds = train_test[\"train\"]\n",
    "    val_ds = test_valid[\"train\"]\n",
    "    test_ds = test_valid[\"test\"]\n",
    "\n",
    "    # Label mapping for discriminative methods\n",
    "    label_list = sorted(list(set(train_ds[\"queue\"])))\n",
    "    label2id = {lab: i for i, lab in enumerate(label_list)}\n",
    "    id2label = {i: lab for lab, i in label2id.items()}\n",
    "\n",
    "    print(\"Label distribution (train):\")\n",
    "    print(Counter(train_ds[\"queue\"]))\n",
    "\n",
    "    return train_ds, val_ds, test_ds, label_list, label2id, id2label\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Load data\n",
    "train_ds, val_ds, test_ds, label_list, label2id, id2label = load_and_prepare_data()\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "##encode_dataset : padding=Flase for more effecient training\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "num_labels = len(label_list)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Encode dataset for BERT\n",
    "# ---------------------------\n",
    "def encode_dataset(ds):\n",
    "    texts = [f\"{subj} {body}\" for subj, body in zip(ds[\"subject\"], ds[\"body\"])]\n",
    "    encodings = tokenizer(texts, truncation=True, padding=False, max_length=512)\n",
    "    encodings[\"labels\"] = [label2id[label] for label in ds[\"queue\"]]\n",
    "    return Dataset.from_dict(encodings)\n",
    "\n",
    "train_enc = encode_dataset(train_ds)\n",
    "val_enc = encode_dataset(val_ds)\n",
    "test_enc = encode_dataset(test_ds)\n",
    "\n",
    "# ---------------------------\n",
    "# DataLoaders\n",
    "# ---------------------------\n",
    "train_loader = DataLoader(train_enc, batch_size=16, collate_fn=data_collator)\n",
    "val_loader = DataLoader(val_enc, batch_size=16)\n",
    "test_loader = DataLoader(test_enc, batch_size=16)\n",
    "\n",
    "# ---------------------------\n",
    "# Optimizer\n",
    "# ---------------------------\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# ---------------------------\n",
    "# Training loop\n",
    "# ---------------------------\n",
    "start_time = time.time()\n",
    "epochs = 4\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "time_cls_train = time.time() - start_time\n",
    "mem_cls_train = torch.cuda.max_memory_allocated(device) / (1024**2) # Convert to MB\n",
    "print(f\"Peak GPU Memory during training: {mem_cls_train:.2f} MB\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "# ---------------------------\n",
    "# Prediction function\n",
    "# ---------------------------\n",
    "start_time = time.time()\n",
    "model.eval()\n",
    "def predict_email(subject, body):\n",
    "    text = f\"{subject} {body}\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        pred_id = torch.argmax(probs, dim=-1).item()\n",
    "        return id2label[pred_id]\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluation\n",
    "# ---------------------------\n",
    "true_labels = [ex[\"queue\"] for ex in test_ds]\n",
    "pred_labels = [predict_email(ex[\"subject\"], ex[\"body\"]) for ex in test_ds]\n",
    "\n",
    "acc_cls = accuracy_score(true_labels, pred_labels)\n",
    "time_cls_inf = time.time() - start_time\n",
    "mem_cls_inf = torch.cuda.max_memory_allocated(device) / (1024**2) # Convert to MB\n",
    "print(f\"memory used: {mem_cls_inf:.2f} MB\")\n",
    "print(f\"Inference Time: {time_cls_inf:.2f}s\")\n",
    "print(f\"Discriminative BERT classifier accuracy: {acc_cls:.4f}\")\n",
    "\n",
    "model.to(\"cpu\")\n",
    "\n",
    "\n",
    "# Force Python's garbage collector to run\n",
    "gc.collect()\n",
    "\n",
    "# Clear the actual VRAM on the GPU\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T18:21:45.193579Z",
     "iopub.status.busy": "2026-01-07T18:21:45.193202Z",
     "iopub.status.idle": "2026-01-07T18:27:10.601928Z",
     "shell.execute_reply": "2026-01-07T18:27:10.601009Z",
     "shell.execute_reply.started": "2026-01-07T18:21:45.193548Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ AWS credentials loaded from Kaggle secrets\n",
      "✓ Authenticated as: arn:aws:iam::998821594730:user/tester\n",
      "✓ Account ID: 998821594730\n",
      "============================================================\n",
      "KAGGLE → AWS SAGEMAKER DEPLOYMENT\n",
      "============================================================\n",
      "\n",
      "[1/7] Setting up AWS credentials...\n",
      "✓ AWS credentials loaded from Kaggle secrets\n",
      "✓ Authenticated as: arn:aws:iam::998821594730:user/tester\n",
      "✓ Account ID: 998821594730\n",
      "\n",
      "[2/7] Saving model...\n",
      "\n",
      "[3/7] Creating inference script...\n",
      "✓ Inference files created in ./deployment_model/code\n",
      "\n",
      "[4/7] Packaging model...\n",
      "\n",
      "[5/7] Uploading to S3...\n",
      "Creating model.tar.gz...\n",
      "✓ Model packaged: 235.86 MB\n",
      "Uploading to S3 (this may take a few minutes)...\n",
      "  Uploaded 1.0 MB\n",
      "✓ Upload complete!\n",
      "  S3 URL: s3://distilbert-routing-emails-ok-2026/distilbert-tickets/model.tar.gz\n",
      "\n",
      "[6/7] Deploying to SageMaker...\n",
      "Creating SageMaker model: distilbert-tickets-model-1767810125\n",
      "Region: eu-north-1\n",
      "Trying image 1/3...\n",
      "✓ Model created with image 1\n",
      "✓ Endpoint config created: distilbert-tickets-config-1767810126\n",
      "Creating endpoint: distilbert-tickets\n",
      "Instance: ml.m5.xlarge\n",
      "⏳ This takes 5-10 minutes. Grab a coffee...\n",
      "Waiting for endpoint to be ready...\n",
      "✓ Deployment successful!\n",
      "  Endpoint: distilbert-tickets\n",
      "\n",
      "[7/7] Testing endpoint...\n",
      "Testing endpoint...\n",
      "Subject: Cannot login to account\n",
      "Body: I tried resetting my password but the email never arrived. Need urgent help!...\n",
      "\n",
      "✓ Prediction successful!\n",
      "[\n",
      "  {\n",
      "    \"predicted_queue\": \"Technical Support\",\n",
      "    \"confidence\": 0.9933032393455505,\n",
      "    \"all_probabilities\": {\n",
      "      \"Billing and Payments\": 0.0003731010656338185,\n",
      "      \"Customer Service\": 0.005060606636106968,\n",
      "      \"General Inquiry\": 0.0008767152321524918,\n",
      "      \"Sales and Pre-Sales\": 0.000386346917366609,\n",
      "      \"Technical Support\": 0.9933032393455505\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      "============================================================\n",
      "✓ DEPLOYMENT COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Endpoint name: distilbert-tickets\n",
      "\n",
      "⚠️  Don't forget to delete the endpoint when done!\n",
      "   Run: delete_endpoint('distilbert-tickets')\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# KAGGLE-READY SAGEMAKER DEPLOYMENT SCRIPT\n",
    "# =====================================================\n",
    "\n",
    "# Install required packages (run in Kaggle notebook cell)\n",
    "# !pip install -q boto3 sagemaker\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "# =====================================================\n",
    "# STEP 1: Load AWS Credentials from Kaggle Secrets\n",
    "# =====================================================\n",
    "\n",
    "def setup_aws_credentials():\n",
    "    \"\"\"\n",
    "    Load AWS credentials from Kaggle secrets\n",
    "    \"\"\"\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    \n",
    "    user_secrets = UserSecretsClient()\n",
    "    \n",
    "    # Get credentials from Kaggle secrets\n",
    "    aws_access_key = \"\"\n",
    "    aws_secret_key = \"\"\n",
    "    \n",
    "    # Set as environment variables\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = aws_access_key\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = aws_secret_key\n",
    "    os.environ['AWS_DEFAULT_REGION'] = 'eu-north-1'  # Change if needed\n",
    "    \n",
    "    print(\"✓ AWS credentials loaded from Kaggle secrets\")\n",
    "    \n",
    "    # Test credentials\n",
    "    import boto3\n",
    "    try:\n",
    "        sts = boto3.client('sts')\n",
    "        identity = sts.get_caller_identity()\n",
    "        print(f\"✓ Authenticated as: {identity['Arn']}\")\n",
    "        print(f\"✓ Account ID: {identity['Account']}\")\n",
    "        return identity['Account']\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Authentication failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run this first!\n",
    "account_id = setup_aws_credentials()\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# STEP 2: Save Model with All Required Files\n",
    "# =====================================================\n",
    "\n",
    "def save_model_for_kaggle(model, tokenizer, label2id, id2label, save_dir='./deployment_model'):\n",
    "    \"\"\"\n",
    "    Save your trained model in the correct format\n",
    "    \"\"\"\n",
    "    print(f\"Saving model to {save_dir}...\")\n",
    "    \n",
    "    # Create directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Move model to CPU before saving (important in Kaggle!)\n",
    "    model.to('cpu')\n",
    "    \n",
    "    # Save model and tokenizer\n",
    "    model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    \n",
    "    # Save label mappings\n",
    "    with open(f'{save_dir}/label_mapping.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'label2id': label2id,\n",
    "            'id2label': {str(k): v for k, v in id2label.items()}\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"✓ Model saved successfully\")\n",
    "    print(f\"✓ Files: {os.listdir(save_dir)}\")\n",
    "    \n",
    "    return save_dir\n",
    "\n",
    "# Example usage (add this after your training code):\n",
    "# save_dir = save_model_for_kaggle(model, tokenizer, label2id, id2label)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# STEP 3: Create Inference Script\n",
    "# =====================================================\n",
    "\n",
    "def create_inference_files(save_dir='./deployment_model'):\n",
    "    \"\"\"\n",
    "    Create inference.py and requirements.txt\n",
    "    \"\"\"\n",
    "    code_dir = f'{save_dir}/code'\n",
    "    os.makedirs(code_dir, exist_ok=True)\n",
    "    \n",
    "    # Inference script\n",
    "    inference_code = '''import json\n",
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "import os\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the model for inference\"\"\"\n",
    "    print(f\"Loading model from {model_dir}\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_dir)\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_dir)\n",
    "    \n",
    "    # Load label mappings\n",
    "    with open(f'{model_dir}/label_mapping.json', 'r') as f:\n",
    "        label_mapping = json.load(f)\n",
    "    id2label = {int(k): v for k, v in label_mapping['id2label'].items()}\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✓ Model loaded on {device}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'tokenizer': tokenizer,\n",
    "        'id2label': id2label,\n",
    "        'device': device\n",
    "    }\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"Parse input data\"\"\"\n",
    "    if request_content_type == 'application/json':\n",
    "        input_data = json.loads(request_body)\n",
    "        if isinstance(input_data, dict):\n",
    "            return [input_data]\n",
    "        elif isinstance(input_data, list):\n",
    "            return input_data\n",
    "        else:\n",
    "            raise ValueError(\"Input must be dict or list of dicts\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model_dict):\n",
    "    \"\"\"Make predictions\"\"\"\n",
    "    model = model_dict['model']\n",
    "    tokenizer = model_dict['tokenizer']\n",
    "    id2label = model_dict['id2label']\n",
    "    device = model_dict['device']\n",
    "    \n",
    "    # Prepare texts (subject + body format from your training)\n",
    "    texts = [f\"{item['subject']} {item['body']}\" for item in input_data]\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        pred_ids = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "        confidences = torch.max(probs, dim=-1).values.cpu().numpy()\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for i, pred_id in enumerate(pred_ids):\n",
    "        results.append({\n",
    "            'predicted_queue': id2label[pred_id],\n",
    "            'confidence': float(confidences[i]),\n",
    "            'all_probabilities': {\n",
    "                id2label[j]: float(probs[i][j].item())\n",
    "                for j in range(len(id2label))\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def output_fn(prediction, response_content_type):\n",
    "    \"\"\"Format output\"\"\"\n",
    "    return json.dumps(prediction)\n",
    "'''\n",
    "    \n",
    "    with open(f'{code_dir}/inference.py', 'w') as f:\n",
    "        f.write(inference_code)\n",
    "    \n",
    "    # Requirements file\n",
    "    requirements = \"\"\"transformers==4.26.0\n",
    "torch==2.0.0\n",
    "\"\"\"\n",
    "    \n",
    "    with open(f'{code_dir}/requirements.txt', 'w') as f:\n",
    "        f.write(requirements)\n",
    "    \n",
    "    print(f\"✓ Inference files created in {code_dir}\")\n",
    "\n",
    "# create_inference_files()\n",
    "\n",
    "# =====================================================\n",
    "# STEP 4: Package and Upload to S3\n",
    "# =====================================================\n",
    "\n",
    "def package_and_upload_to_s3(save_dir='./deployment_model',bucket_name=\"distilbert-routing-emails-ok-2026\",s3_prefix='distilbert-tickets'):\n",
    "    \"\"\"\n",
    "    Package model and upload to S3 from Kaggle\n",
    "    \"\"\"\n",
    "    import boto3\n",
    "    \n",
    "    # Create tar.gz\n",
    "    tar_filename = 'model.tar.gz'\n",
    "    print(f\"Creating {tar_filename}...\")\n",
    "    \n",
    "    with tarfile.open(tar_filename, 'w:gz') as tar:\n",
    "        tar.add(save_dir, arcname='.')\n",
    "    \n",
    "    file_size = os.path.getsize(tar_filename) / (1024*1024)\n",
    "    print(f\"✓ Model packaged: {file_size:.2f} MB\")\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_key = f'{s3_prefix}/model.tar.gz'\n",
    "    \n",
    "    print(f\"Uploading to S3 (this may take a few minutes)...\")\n",
    "    s3_client.upload_file(\n",
    "        tar_filename, \n",
    "        bucket_name, \n",
    "        s3_key,\n",
    "        Callback=lambda bytes_transferred: print(f\"  Uploaded {bytes_transferred / (1024*1024):.1f} MB\", end='\\r')\n",
    "    )\n",
    "    \n",
    "    model_data_url = f's3://{bucket_name}/{s3_key}'\n",
    "    print(f\"\\n✓ Upload complete!\")\n",
    "    print(f\"  S3 URL: {model_data_url}\")\n",
    "    \n",
    "    return model_data_url\n",
    "\n",
    "# Example:\n",
    "# model_url = package_and_upload_to_s3(bucket_name='my-sagemaker-bucket')\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# STEP 5: Deploy to SageMaker\n",
    "# =====================================================\n",
    "\n",
    "def deploy_model_from_kaggle(model_data_url,\n",
    "                              role_arn,\n",
    "                              endpoint_name='distilbert-tickets',\n",
    "                              instance_type='ml.m5.xlarge'):\n",
    "    import boto3\n",
    "    import time\n",
    "    \n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    region = boto3.Session().region_name\n",
    "    \n",
    "    # Try different PyTorch + Transformers versions\n",
    "    images_to_try = [\n",
    "        f\"763104351884.dkr.ecr.eu-north-1.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-cpu-py310-ubuntu20.04\",\n",
    "        f\"763104351884.dkr.ecr.eu-north-1.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-cpu-py39-ubuntu20.04\",\n",
    "        f\"763104351884.dkr.ecr.eu-north-1.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-cpu-py38-ubuntu20.04\",\n",
    "    ]\n",
    "    \n",
    "    model_name = f\"{endpoint_name}-model-{int(time.time())}\"\n",
    "    \n",
    "    print(f\"Creating SageMaker model: {model_name}\")\n",
    "    print(f\"Region: {region}\")\n",
    "    \n",
    "    # Try each image\n",
    "    for i, image_uri in enumerate(images_to_try, 1):\n",
    "        print(f\"Trying image {i}/{len(images_to_try)}...\")\n",
    "        try:\n",
    "            # Create model\n",
    "            sagemaker_client.create_model(\n",
    "                ModelName=model_name,\n",
    "                PrimaryContainer={\n",
    "                    'Image': image_uri,\n",
    "                    'ModelDataUrl': model_data_url,\n",
    "                    'Environment': {\n",
    "                        'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "                        'SAGEMAKER_SUBMIT_DIRECTORY': model_data_url\n",
    "                    }\n",
    "                },\n",
    "                ExecutionRoleArn=role_arn\n",
    "            )\n",
    "            print(f\"✓ Model created with image {i}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if i < len(images_to_try):\n",
    "                print(f\"  Image {i} not available, trying next...\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"✗ All images failed: {e}\")\n",
    "                return None\n",
    "    \n",
    "    try:\n",
    "        # Create endpoint config\n",
    "        endpoint_config_name = f\"{endpoint_name}-config-{int(time.time())}\"\n",
    "        sagemaker_client.create_endpoint_config(\n",
    "            EndpointConfigName=endpoint_config_name,\n",
    "            ProductionVariants=[{\n",
    "                'VariantName': 'AllTraffic',\n",
    "                'ModelName': model_name,\n",
    "                'InitialInstanceCount': 1,\n",
    "                'InstanceType': instance_type\n",
    "            }]\n",
    "        )\n",
    "        print(f\"✓ Endpoint config created: {endpoint_config_name}\")\n",
    "        \n",
    "        # Create endpoint\n",
    "        print(f\"Creating endpoint: {endpoint_name}\")\n",
    "        print(f\"Instance: {instance_type}\")\n",
    "        print(\"⏳ This takes 5-10 minutes. Grab a coffee...\")\n",
    "        \n",
    "        sagemaker_client.create_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            EndpointConfigName=endpoint_config_name\n",
    "        )\n",
    "        \n",
    "        # Wait for endpoint\n",
    "        print(\"Waiting for endpoint to be ready...\")\n",
    "        waiter = sagemaker_client.get_waiter('endpoint_in_service')\n",
    "        waiter.wait(EndpointName=endpoint_name)\n",
    "        \n",
    "        print(f\"✓ Deployment successful!\")\n",
    "        print(f\"  Endpoint: {endpoint_name}\")\n",
    "        return endpoint_name\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Deployment failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example:\n",
    "# role = f'arn:aws:iam::{account_id}:role/SageMakerExecutionRole'\n",
    "# predictor = deploy_model_from_kaggle(model_url, role)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# STEP 6: Test Endpoint from Kaggle\n",
    "# =====================================================\n",
    "\n",
    "def test_endpoint_from_kaggle(endpoint_name='distilbert-tickets'):\n",
    "    \"\"\"\n",
    "    Test your deployed endpoint\n",
    "    \"\"\"\n",
    "    import boto3\n",
    "    import json\n",
    "    \n",
    "    runtime = boto3.client('sagemaker-runtime')\n",
    "    \n",
    "    # Test with a sample ticket\n",
    "    test_ticket = {\n",
    "        \"subject\": \"Cannot login to account\",\n",
    "        \"body\": \"I tried resetting my password but the email never arrived. Need urgent help!\"\n",
    "    }\n",
    "    \n",
    "    print(\"Testing endpoint...\")\n",
    "    print(f\"Subject: {test_ticket['subject']}\")\n",
    "    print(f\"Body: {test_ticket['body'][:100]}...\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        response = runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(test_ticket)\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response['Body'].read().decode())\n",
    "        \n",
    "        print(\"✓ Prediction successful!\")\n",
    "        print(json.dumps(result, indent=2))\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Prediction failed: {e}\")\n",
    "        return None\n",
    "# test_endpoint_from_kaggle()\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# COMPLETE WORKFLOW FOR KAGGLE\n",
    "# =====================================================\n",
    "\n",
    "def complete_kaggle_deployment():\n",
    "    \"\"\"\n",
    "    Full deployment workflow for Kaggle notebooks\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"KAGGLE → AWS SAGEMAKER DEPLOYMENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # CONFIGURATION - CHANGE THESE!\n",
    "    BUCKET_NAME=\"distilbert-routing-emails-ok-2026\"\n",
    "    ENDPOINT_NAME = 'distilbert-tickets'\n",
    "    \n",
    "    # Step 1: Setup credentials\n",
    "    print(\"\\n[1/7] Setting up AWS credentials...\")\n",
    "    account_id = setup_aws_credentials()\n",
    "    if not account_id:\n",
    "        print(\"✗ Failed to authenticate. Check your Kaggle secrets!\")\n",
    "        return\n",
    "    \n",
    "    # Construct role ARN\n",
    "    ROLE_ARN =\"arn:aws:iam::998821594730:role/service-role/AmazonSageMaker-ExecutionRole-20260106T015680\"\n",
    "    \n",
    "    # Step 2: Save model\n",
    "    print(\"\\n[2/7] Saving model...\")\n",
    "    # Assuming model, tokenizer, label2id, id2label are in scope\n",
    "    # save_dir = save_model_for_kaggle(model, tokenizer, label2id, id2label)\n",
    "    save_dir = './deployment_model'  # If already saved\n",
    "    \n",
    "    # Step 3: Create inference files\n",
    "    print(\"\\n[3/7] Creating inference script...\")\n",
    "    create_inference_files(save_dir)\n",
    "    \n",
    "    # Step 4: Package model\n",
    "    print(\"\\n[4/7] Packaging model...\")\n",
    "    # (packaging happens in upload step)\n",
    "    \n",
    "    # Step 5: Upload to S3\n",
    "    print(\"\\n[5/7] Uploading to S3...\")\n",
    "    model_url = package_and_upload_to_s3(save_dir, BUCKET_NAME)\n",
    "    \n",
    "    # Step 6: Deploy\n",
    "    print(\"\\n[6/7] Deploying to SageMaker...\")\n",
    "    predictor = deploy_model_from_kaggle(model_url, ROLE_ARN, ENDPOINT_NAME)\n",
    "    \n",
    "    if predictor:\n",
    "        # Step 7: Test\n",
    "        print(\"\\n[7/7] Testing endpoint...\")\n",
    "        test_endpoint_from_kaggle(ENDPOINT_NAME)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✓ DEPLOYMENT COMPLETE!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nEndpoint name: {ENDPOINT_NAME}\")\n",
    "        print(\"\\n⚠️  Don't forget to delete the endpoint when done!\")\n",
    "        print(f\"   Run: delete_endpoint('{ENDPOINT_NAME}')\")\n",
    "    else:\n",
    "        print(\"\\n✗ Deployment failed. Check the errors above.\")\n",
    "\n",
    "# Uncomment to run:\n",
    "complete_kaggle_deployment()\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# UTILITY: Delete Endpoint\n",
    "# =====================================================\n",
    "\n",
    "def delete_endpoint(endpoint_name='distilbert-tickets'):\n",
    "    \"\"\"\n",
    "    Delete endpoint to stop charges\n",
    "    \"\"\"\n",
    "    import boto3\n",
    "    \n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    \n",
    "    try:\n",
    "        print(f\"Deleting endpoint: {endpoint_name}...\")\n",
    "        sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "        print(\"✓ Endpoint deleted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to delete endpoint: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T18:31:59.670757Z",
     "iopub.status.busy": "2026-01-07T18:31:59.670304Z",
     "iopub.status.idle": "2026-01-07T18:32:00.313038Z",
     "shell.execute_reply": "2026-01-07T18:32:00.312133Z",
     "shell.execute_reply.started": "2026-01-07T18:31:59.670721Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting endpoint: distilbert-tickets...\n",
      "✓ Endpoint deleted successfully\n"
     ]
    }
   ],
   "source": [
    "delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMlByc/+fl/tpg+zHc7SjDe",
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
